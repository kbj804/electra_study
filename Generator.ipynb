{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c84b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "    # 임베딩사이즈 설정 안되어있으면 BEERT의 config hiddensize 적용\n",
    "    embedding_size = (\n",
    "        self._bert_config.hidden_size if config.embedding_size is None else\n",
    "        config.embedding_size)\n",
    "    cloze_output = None\n",
    "    if config.uniform_generator:\n",
    "      # simple generator sampling fakes uniformly at random\n",
    "      mlm_output = self._get_masked_lm_output(masked_inputs, None)\n",
    "    elif ((config.electra_objective or config.electric_objective)\n",
    "          and config.untied_generator):\n",
    "      generator_config = get_generator_config(config, self._bert_config)\n",
    "      if config.two_tower_generator:\n",
    "        # two-tower cloze model generator used for electric\n",
    "        generator = TwoTowerClozeTransformer(\n",
    "            config, generator_config, unmasked_inputs, is_training,\n",
    "            embedding_size)\n",
    "        cloze_output = self._get_cloze_outputs(unmasked_inputs, generator)\n",
    "        mlm_output = get_softmax_output(\n",
    "            pretrain_helpers.gather_positions(\n",
    "                cloze_output.logits, masked_inputs.masked_lm_positions),\n",
    "            masked_inputs.masked_lm_ids, masked_inputs.masked_lm_weights,\n",
    "            self._bert_config.vocab_size)\n",
    "      else:\n",
    "        # small masked language model generator\n",
    "        generator = build_transformer(\n",
    "            config, masked_inputs, is_training, generator_config,\n",
    "            embedding_size=(None if config.untied_generator_embeddings\n",
    "                            else embedding_size),\n",
    "            untied_embeddings=config.untied_generator_embeddings,\n",
    "            scope=\"generator\")\n",
    "        mlm_output = self._get_masked_lm_output(masked_inputs, generator)\n",
    "    else:\n",
    "      # full-sized masked language model generator if using BERT objective or if\n",
    "      # the generator and discriminator have tied weights\n",
    "      generator = build_transformer(\n",
    "          config, masked_inputs, is_training, self._bert_config,\n",
    "          embedding_size=embedding_size)\n",
    "      mlm_output = self._get_masked_lm_output(masked_inputs, generator)\n",
    "    fake_data = self._get_fake_data(masked_inputs, mlm_output.logits)\n",
    "    self.mlm_output = mlm_output\n",
    "    self.total_loss = config.gen_weight * (\n",
    "        cloze_output.loss if config.two_tower_generator else mlm_output.loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
