{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950db56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Pre-trains an ELECTRA model.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import configure_pretraining\n",
    "from model import modeling\n",
    "from model import optimization\n",
    "from pretrain import pretrain_data\n",
    "from pretrain import pretrain_helpers\n",
    "from util import training_utils\n",
    "from util import utils\n",
    "\n",
    "\n",
    "class PretrainingModel(object):\n",
    "  \"\"\"Transformer pre-training using the replaced-token-detection task.\"\"\"\n",
    "\n",
    "  def __init__(self, config: configure_pretraining.PretrainingConfig,\n",
    "               features, is_training):\n",
    "    # Set up model config\n",
    "    self._config = config\n",
    "    self._bert_config = training_utils.get_bert_config(config)\n",
    "    if config.debug:\n",
    "      self._bert_config.num_hidden_layers = 3\n",
    "      self._bert_config.hidden_size = 144\n",
    "      self._bert_config.intermediate_size = 144 * 4\n",
    "      self._bert_config.num_attention_heads = 4\n",
    "\n",
    "    # Mask the input\n",
    "    unmasked_inputs = pretrain_data.features_to_inputs(features)\n",
    "    masked_inputs = pretrain_helpers.mask(\n",
    "        config, unmasked_inputs, config.mask_prob)\n",
    "\n",
    "    # Generator\n",
    "    embedding_size = (\n",
    "        self._bert_config.hidden_size if config.embedding_size is None else\n",
    "        config.embedding_size)\n",
    "    cloze_output = None\n",
    "    if config.uniform_generator:\n",
    "      # simple generator sampling fakes uniformly at random\n",
    "      mlm_output = self._get_masked_lm_output(masked_inputs, None)\n",
    "    elif ((config.electra_objective or config.electric_objective)\n",
    "          and config.untied_generator):\n",
    "      generator_config = get_generator_config(config, self._bert_config)\n",
    "      if config.two_tower_generator:\n",
    "        # two-tower cloze model generator used for electric\n",
    "        generator = TwoTowerClozeTransformer(\n",
    "            config, generator_config, unmasked_inputs, is_training,\n",
    "            embedding_size)\n",
    "        cloze_output = self._get_cloze_outputs(unmasked_inputs, generator)\n",
    "        mlm_output = get_softmax_output(\n",
    "            pretrain_helpers.gather_positions(\n",
    "                cloze_output.logits, masked_inputs.masked_lm_positions),\n",
    "            masked_inputs.masked_lm_ids, masked_inputs.masked_lm_weights,\n",
    "            self._bert_config.vocab_size)\n",
    "      else:\n",
    "        # small masked language model generator\n",
    "        generator = build_transformer(\n",
    "            config, masked_inputs, is_training, generator_config,\n",
    "            embedding_size=(None if config.untied_generator_embeddings\n",
    "                            else embedding_size),\n",
    "            untied_embeddings=config.untied_generator_embeddings,\n",
    "            scope=\"generator\")\n",
    "        mlm_output = self._get_masked_lm_output(masked_inputs, generator)\n",
    "    else:\n",
    "      # full-sized masked language model generator if using BERT objective or if\n",
    "      # the generator and discriminator have tied weights\n",
    "      generator = build_transformer(\n",
    "          config, masked_inputs, is_training, self._bert_config,\n",
    "          embedding_size=embedding_size)\n",
    "      mlm_output = self._get_masked_lm_output(masked_inputs, generator)\n",
    "    fake_data = self._get_fake_data(masked_inputs, mlm_output.logits)\n",
    "    self.mlm_output = mlm_output\n",
    "    self.total_loss = config.gen_weight * (\n",
    "        cloze_output.loss if config.two_tower_generator else mlm_output.loss)\n",
    "\n",
    "    # Discriminator\n",
    "    disc_output = None\n",
    "    if config.electra_objective or config.electric_objective:\n",
    "      discriminator = build_transformer(\n",
    "          config, fake_data.inputs, is_training, self._bert_config,\n",
    "          reuse=not config.untied_generator, embedding_size=embedding_size)\n",
    "      disc_output = self._get_discriminator_output(\n",
    "          fake_data.inputs, discriminator, fake_data.is_fake_tokens,\n",
    "          cloze_output)\n",
    "      self.total_loss += config.disc_weight * disc_output.loss\n",
    "\n",
    "    # Evaluation\n",
    "    eval_fn_inputs = {\n",
    "        \"input_ids\": masked_inputs.input_ids,\n",
    "        \"masked_lm_preds\": mlm_output.preds,\n",
    "        \"mlm_loss\": mlm_output.per_example_loss,\n",
    "        \"masked_lm_ids\": masked_inputs.masked_lm_ids,\n",
    "        \"masked_lm_weights\": masked_inputs.masked_lm_weights,\n",
    "        \"input_mask\": masked_inputs.input_mask\n",
    "    }\n",
    "    if config.electra_objective or config.electric_objective:\n",
    "      eval_fn_inputs.update({\n",
    "          \"disc_loss\": disc_output.per_example_loss,\n",
    "          \"disc_labels\": disc_output.labels,\n",
    "          \"disc_probs\": disc_output.probs,\n",
    "          \"disc_preds\": disc_output.preds,\n",
    "          \"sampled_tokids\": tf.argmax(fake_data.sampled_tokens, -1,\n",
    "                                      output_type=tf.int32)\n",
    "      })\n",
    "    eval_fn_keys = eval_fn_inputs.keys()\n",
    "    eval_fn_values = [eval_fn_inputs[k] for k in eval_fn_keys]\n",
    "\n",
    "    def metric_fn(*args):\n",
    "      \"\"\"Computes the loss and accuracy of the model.\"\"\"\n",
    "      d = {k: arg for k, arg in zip(eval_fn_keys, args)}\n",
    "      metrics = dict()\n",
    "      metrics[\"masked_lm_accuracy\"] = tf.metrics.accuracy(\n",
    "          labels=tf.reshape(d[\"masked_lm_ids\"], [-1]),\n",
    "          predictions=tf.reshape(d[\"masked_lm_preds\"], [-1]),\n",
    "          weights=tf.reshape(d[\"masked_lm_weights\"], [-1]))\n",
    "      metrics[\"masked_lm_loss\"] = tf.metrics.mean(\n",
    "          values=tf.reshape(d[\"mlm_loss\"], [-1]),\n",
    "          weights=tf.reshape(d[\"masked_lm_weights\"], [-1]))\n",
    "      if config.electra_objective or config.electric_objective:\n",
    "        metrics[\"sampled_masked_lm_accuracy\"] = tf.metrics.accuracy(\n",
    "            labels=tf.reshape(d[\"masked_lm_ids\"], [-1]),\n",
    "            predictions=tf.reshape(d[\"sampled_tokids\"], [-1]),\n",
    "            weights=tf.reshape(d[\"masked_lm_weights\"], [-1]))\n",
    "        if config.disc_weight > 0:\n",
    "          metrics[\"disc_loss\"] = tf.metrics.mean(d[\"disc_loss\"])\n",
    "          metrics[\"disc_auc\"] = tf.metrics.auc(\n",
    "              d[\"disc_labels\"] * d[\"input_mask\"],\n",
    "              d[\"disc_probs\"] * tf.cast(d[\"input_mask\"], tf.float32))\n",
    "          metrics[\"disc_accuracy\"] = tf.metrics.accuracy(\n",
    "              labels=d[\"disc_labels\"], predictions=d[\"disc_preds\"],\n",
    "              weights=d[\"input_mask\"])\n",
    "          metrics[\"disc_precision\"] = tf.metrics.accuracy(\n",
    "              labels=d[\"disc_labels\"], predictions=d[\"disc_preds\"],\n",
    "              weights=d[\"disc_preds\"] * d[\"input_mask\"])\n",
    "          metrics[\"disc_recall\"] = tf.metrics.accuracy(\n",
    "              labels=d[\"disc_labels\"], predictions=d[\"disc_preds\"],\n",
    "              weights=d[\"disc_labels\"] * d[\"input_mask\"])\n",
    "      return metrics\n",
    "    self.eval_metrics = (metric_fn, eval_fn_values)\n",
    "\n",
    "  def _get_masked_lm_output(self, inputs: pretrain_data.Inputs, model):\n",
    "    \"\"\"Masked language modeling softmax layer.\"\"\"\n",
    "    with tf.variable_scope(\"generator_predictions\"):\n",
    "      if self._config.uniform_generator:\n",
    "        logits = tf.zeros(self._bert_config.vocab_size)\n",
    "        logits_tiled = tf.zeros(\n",
    "            modeling.get_shape_list(inputs.masked_lm_ids) +\n",
    "            [self._bert_config.vocab_size])\n",
    "        logits_tiled += tf.reshape(logits, [1, 1, self._bert_config.vocab_size])\n",
    "        logits = logits_tiled\n",
    "      else:\n",
    "        relevant_reprs = pretrain_helpers.gather_positions(\n",
    "            model.get_sequence_output(), inputs.masked_lm_positions)\n",
    "        logits = get_token_logits(\n",
    "            relevant_reprs, model.get_embedding_table(), self._bert_config)\n",
    "      return get_softmax_output(\n",
    "          logits, inputs.masked_lm_ids, inputs.masked_lm_weights,\n",
    "          self._bert_config.vocab_size)\n",
    "\n",
    "  def _get_discriminator_output(\n",
    "      self, inputs, discriminator, labels, cloze_output=None):\n",
    "    \"\"\"Discriminator binary classifier.\"\"\"\n",
    "    with tf.variable_scope(\"discriminator_predictions\"):\n",
    "      hidden = tf.layers.dense(\n",
    "          discriminator.get_sequence_output(),\n",
    "          units=self._bert_config.hidden_size,\n",
    "          activation=modeling.get_activation(self._bert_config.hidden_act),\n",
    "          kernel_initializer=modeling.create_initializer(\n",
    "              self._bert_config.initializer_range))\n",
    "      logits = tf.squeeze(tf.layers.dense(hidden, units=1), -1)\n",
    "      if self._config.electric_objective:\n",
    "        log_q = tf.reduce_sum(\n",
    "            tf.nn.log_softmax(cloze_output.logits) * tf.one_hot(\n",
    "                inputs.input_ids, depth=self._bert_config.vocab_size,\n",
    "                dtype=tf.float32), -1)\n",
    "        log_q = tf.stop_gradient(log_q)\n",
    "        logits += log_q\n",
    "        logits += tf.log(self._config.mask_prob / (1 - self._config.mask_prob))\n",
    "\n",
    "      weights = tf.cast(inputs.input_mask, tf.float32)\n",
    "      labelsf = tf.cast(labels, tf.float32)\n",
    "      losses = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=logits, labels=labelsf) * weights\n",
    "      per_example_loss = (tf.reduce_sum(losses, axis=-1) /\n",
    "                          (1e-6 + tf.reduce_sum(weights, axis=-1)))\n",
    "      loss = tf.reduce_sum(losses) / (1e-6 + tf.reduce_sum(weights))\n",
    "      probs = tf.nn.sigmoid(logits)\n",
    "      preds = tf.cast(tf.round((tf.sign(logits) + 1) / 2), tf.int32)\n",
    "      DiscOutput = collections.namedtuple(\n",
    "          \"DiscOutput\", [\"loss\", \"per_example_loss\", \"probs\", \"preds\",\n",
    "                         \"labels\"])\n",
    "      return DiscOutput(\n",
    "          loss=loss, per_example_loss=per_example_loss, probs=probs,\n",
    "          preds=preds, labels=labels,\n",
    "      )\n",
    "\n",
    "  def _get_fake_data(self, inputs, mlm_logits):\n",
    "    \"\"\"Sample from the generator to create corrupted input.\"\"\"\n",
    "    inputs = pretrain_helpers.unmask(inputs)\n",
    "    disallow = tf.one_hot(\n",
    "        inputs.masked_lm_ids, depth=self._bert_config.vocab_size,\n",
    "        dtype=tf.float32) if self._config.disallow_correct else None\n",
    "    sampled_tokens = tf.stop_gradient(pretrain_helpers.sample_from_softmax(\n",
    "        mlm_logits / self._config.temperature, disallow=disallow))\n",
    "    sampled_tokids = tf.argmax(sampled_tokens, -1, output_type=tf.int32)\n",
    "    updated_input_ids, masked = pretrain_helpers.scatter_update(\n",
    "        inputs.input_ids, sampled_tokids, inputs.masked_lm_positions)\n",
    "    if self._config.electric_objective:\n",
    "      labels = masked\n",
    "    else:\n",
    "      labels = masked * (1 - tf.cast(\n",
    "          tf.equal(updated_input_ids, inputs.input_ids), tf.int32))\n",
    "    updated_inputs = pretrain_data.get_updated_inputs(\n",
    "        inputs, input_ids=updated_input_ids)\n",
    "    FakedData = collections.namedtuple(\"FakedData\", [\n",
    "        \"inputs\", \"is_fake_tokens\", \"sampled_tokens\"])\n",
    "    return FakedData(inputs=updated_inputs, is_fake_tokens=labels,\n",
    "                     sampled_tokens=sampled_tokens)\n",
    "\n",
    "  def _get_cloze_outputs(self, inputs: pretrain_data.Inputs, model):\n",
    "    \"\"\"Cloze model softmax layer.\"\"\"\n",
    "    weights = tf.cast(pretrain_helpers.get_candidates_mask(\n",
    "        self._config, inputs), tf.float32)\n",
    "    with tf.variable_scope(\"cloze_predictions\"):\n",
    "      logits = get_token_logits(model.get_sequence_output(),\n",
    "                                model.get_embedding_table(), self._bert_config)\n",
    "      return get_softmax_output(logits, inputs.input_ids, weights,\n",
    "                                self._bert_config.vocab_size)\n",
    "\n",
    "\n",
    "def get_token_logits(input_reprs, embedding_table, bert_config):\n",
    "  hidden = tf.layers.dense(\n",
    "      input_reprs,\n",
    "      units=modeling.get_shape_list(embedding_table)[-1],\n",
    "      activation=modeling.get_activation(bert_config.hidden_act),\n",
    "      kernel_initializer=modeling.create_initializer(\n",
    "          bert_config.initializer_range))\n",
    "  hidden = modeling.layer_norm(hidden)\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\",\n",
    "      shape=[bert_config.vocab_size],\n",
    "      initializer=tf.zeros_initializer())\n",
    "  logits = tf.matmul(hidden, embedding_table, transpose_b=True)\n",
    "  logits = tf.nn.bias_add(logits, output_bias)\n",
    "  return logits\n",
    "\n",
    "\n",
    "def get_softmax_output(logits, targets, weights, vocab_size):\n",
    "  oh_labels = tf.one_hot(targets, depth=vocab_size, dtype=tf.float32)\n",
    "  preds = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "  probs = tf.nn.softmax(logits)\n",
    "  log_probs = tf.nn.log_softmax(logits)\n",
    "  label_log_probs = -tf.reduce_sum(log_probs * oh_labels, axis=-1)\n",
    "  numerator = tf.reduce_sum(weights * label_log_probs)\n",
    "  denominator = tf.reduce_sum(weights) + 1e-6\n",
    "  loss = numerator / denominator\n",
    "  SoftmaxOutput = collections.namedtuple(\n",
    "      \"SoftmaxOutput\", [\"logits\", \"probs\", \"loss\", \"per_example_loss\", \"preds\",\n",
    "                        \"weights\"])\n",
    "  return SoftmaxOutput(\n",
    "      logits=logits, probs=probs, per_example_loss=label_log_probs,\n",
    "      loss=loss, preds=preds, weights=weights)\n",
    "\n",
    "\n",
    "class TwoTowerClozeTransformer(object):\n",
    "  \"\"\"Build a two-tower Transformer used as Electric's generator.\"\"\"\n",
    "\n",
    "  def __init__(self, config, bert_config, inputs: pretrain_data.Inputs,\n",
    "               is_training, embedding_size):\n",
    "    ltr = build_transformer(\n",
    "        config, inputs, is_training, bert_config,\n",
    "        untied_embeddings=config.untied_generator_embeddings,\n",
    "        embedding_size=(None if config.untied_generator_embeddings\n",
    "                        else embedding_size),\n",
    "        scope=\"generator_ltr\", ltr=True)\n",
    "    rtl = build_transformer(\n",
    "        config, inputs, is_training, bert_config,\n",
    "        untied_embeddings=config.untied_generator_embeddings,\n",
    "        embedding_size=(None if config.untied_generator_embeddings\n",
    "                        else embedding_size),\n",
    "        scope=\"generator_rtl\", rtl=True)\n",
    "    ltr_reprs = ltr.get_sequence_output()\n",
    "    rtl_reprs = rtl.get_sequence_output()\n",
    "    self._sequence_output = tf.concat([roll(ltr_reprs, -1),\n",
    "                                       roll(rtl_reprs, 1)], -1)\n",
    "    self._embedding_table = ltr.embedding_table\n",
    "\n",
    "  def get_sequence_output(self):\n",
    "    return self._sequence_output\n",
    "\n",
    "  def get_embedding_table(self):\n",
    "    return self._embedding_table\n",
    "\n",
    "\n",
    "def build_transformer(config: configure_pretraining.PretrainingConfig,\n",
    "                      inputs: pretrain_data.Inputs, is_training,\n",
    "                      bert_config, reuse=False, **kwargs):\n",
    "  \"\"\"Build a transformer encoder network.\"\"\"\n",
    "  with tf.variable_scope(tf.get_variable_scope(), reuse=reuse):\n",
    "    return modeling.BertModel(\n",
    "        bert_config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=inputs.input_ids,\n",
    "        input_mask=inputs.input_mask,\n",
    "        token_type_ids=inputs.segment_ids,\n",
    "        use_one_hot_embeddings=config.use_tpu,\n",
    "        **kwargs)\n",
    "\n",
    "\n",
    "def roll(arr, direction):\n",
    "  \"\"\"Shifts embeddings in a [batch, seq_len, dim] tensor to the right/left.\"\"\"\n",
    "  return tf.concat([arr[:, direction:, :], arr[:, :direction, :]], axis=1)\n",
    "\n",
    "\n",
    "def get_generator_config(config: configure_pretraining.PretrainingConfig,\n",
    "                         bert_config: modeling.BertConfig):\n",
    "  \"\"\"Get model config for the generator network.\"\"\"\n",
    "  gen_config = modeling.BertConfig.from_dict(bert_config.to_dict())\n",
    "  gen_config.hidden_size = int(round(\n",
    "      bert_config.hidden_size * config.generator_hidden_size))\n",
    "  gen_config.num_hidden_layers = int(round(\n",
    "      bert_config.num_hidden_layers * config.generator_layers))\n",
    "  gen_config.intermediate_size = 4 * gen_config.hidden_size\n",
    "  gen_config.num_attention_heads = max(1, gen_config.hidden_size // 64)\n",
    "  return gen_config\n",
    "\n",
    "\n",
    "def model_fn_builder(config: configure_pretraining.PretrainingConfig):\n",
    "  \"\"\"Build the model for training.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):\n",
    "    \"\"\"Build the model for training.\"\"\"\n",
    "    model = PretrainingModel(config, features,\n",
    "                             mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    utils.log(\"Model is built!\")\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      train_op = optimization.create_optimizer(\n",
    "          model.total_loss, config.learning_rate, config.num_train_steps,\n",
    "          weight_decay_rate=config.weight_decay_rate,\n",
    "          use_tpu=config.use_tpu,\n",
    "          warmup_steps=config.num_warmup_steps,\n",
    "          lr_decay_power=config.lr_decay_power\n",
    "      )\n",
    "      output_spec = tf.estimator.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=model.total_loss,\n",
    "          train_op=train_op,\n",
    "          training_hooks=[training_utils.ETAHook(\n",
    "              {} if config.use_tpu else dict(loss=model.total_loss),\n",
    "              config.num_train_steps, config.iterations_per_loop,\n",
    "              config.use_tpu)]\n",
    "      )\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "      output_spec = tf.estimator.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=model.total_loss,\n",
    "          eval_metrics=model.eval_metrics,\n",
    "          evaluation_hooks=[training_utils.ETAHook(\n",
    "              {} if config.use_tpu else dict(loss=model.total_loss),\n",
    "              config.num_eval_steps, config.iterations_per_loop,\n",
    "              config.use_tpu, is_training=False)])\n",
    "    else:\n",
    "      raise ValueError(\"Only TRAIN and EVAL modes are supported\")\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn\n",
    "\n",
    "\n",
    "def train_or_eval(config: configure_pretraining.PretrainingConfig):\n",
    "  \"\"\"Run pre-training or evaluate the pre-trained model.\"\"\"\n",
    "  if config.do_train == config.do_eval:\n",
    "    raise ValueError(\"Exactly one of `do_train` or `do_eval` must be True.\")\n",
    "  if config.debug and config.do_train:\n",
    "    utils.rmkdir(config.model_dir)\n",
    "  utils.heading(\"Config:\")\n",
    "  utils.log_config(config)\n",
    "\n",
    "  is_per_host = tf.estimator.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  tpu_cluster_resolver = None\n",
    "  if config.use_tpu and config.tpu_name:\n",
    "    tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "        config.tpu_name, zone=config.tpu_zone, project=config.gcp_project)\n",
    "  tpu_config = tf.estimator.tpu.TPUConfig(\n",
    "      iterations_per_loop=config.iterations_per_loop,\n",
    "      num_shards=config.num_tpu_cores,\n",
    "      tpu_job_name=config.tpu_job_name,\n",
    "      per_host_input_for_training=is_per_host)\n",
    "  run_config = tf.estimator.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      model_dir=config.model_dir,\n",
    "      save_checkpoints_steps=config.save_checkpoints_steps,\n",
    "      keep_checkpoint_max=config.keep_checkpoint_max,\n",
    "      tpu_config=tpu_config)\n",
    "  model_fn = model_fn_builder(config=config)\n",
    "  estimator = tf.estimator.tpu.TPUEstimator(\n",
    "      use_tpu=config.use_tpu,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      train_batch_size=config.train_batch_size,\n",
    "      eval_batch_size=config.eval_batch_size)\n",
    "\n",
    "  if config.do_train:\n",
    "    utils.heading(\"Running training\")\n",
    "    estimator.train(input_fn=pretrain_data.get_input_fn(config, True),\n",
    "                    max_steps=config.num_train_steps)\n",
    "  if config.do_eval:\n",
    "    utils.heading(\"Running evaluation\")\n",
    "    result = estimator.evaluate(\n",
    "        input_fn=pretrain_data.get_input_fn(config, False),\n",
    "        steps=config.num_eval_steps)\n",
    "    for key in sorted(result.keys()):\n",
    "      utils.log(\"  {:} = {:}\".format(key, str(result[key])))\n",
    "    return result\n",
    "\n",
    "\n",
    "def train_one_step(config: configure_pretraining.PretrainingConfig):\n",
    "  \"\"\"Builds an ELECTRA model an trains it for one step; useful for debugging.\"\"\"\n",
    "  train_input_fn = pretrain_data.get_input_fn(config, True)\n",
    "  features = tf.data.make_one_shot_iterator(train_input_fn(dict(\n",
    "      batch_size=config.train_batch_size))).get_next()\n",
    "  model = PretrainingModel(config, features, True)\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    utils.log(sess.run(model.total_loss))\n",
    "\n",
    "\n",
    "def main():\n",
    "  parser = argparse.ArgumentParser(description=__doc__)\n",
    "  parser.add_argument(\"--data-dir\", required=True,\n",
    "                      help=\"Location of data files (model weights, etc).\")\n",
    "  parser.add_argument(\"--model-name\", required=True,\n",
    "                      help=\"The name of the model being fine-tuned.\")\n",
    "  parser.add_argument(\"--hparams\", default=\"{}\",\n",
    "                      help=\"JSON dict of model hyperparameters.\")\n",
    "  args = parser.parse_args()\n",
    "  if args.hparams.endswith(\".json\"):\n",
    "    hparams = utils.load_json(args.hparams)\n",
    "  else:\n",
    "    hparams = json.loads(args.hparams)\n",
    "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "  train_or_eval(configure_pretraining.PretrainingConfig(\n",
    "      args.model_name, args.data_dir, **hparams))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
